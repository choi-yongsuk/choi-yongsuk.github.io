---
layout: single
title:  "Pre-training과 supervisior fine-tuning을 조합한  NLP의 다루기"
date: 2020-03-08 12:14:00 +0800
categories: [Blogging, Configuration]
tags: [Jekyll, MathJax]
use_math: true
---

### The Challenge with Generating Coherent Text

시퀀스나 토근 분류와 같은 작업에 특화된 헤드(task-specific heads)에서 예측 생성은 매우 간단하다. 모델이 일련의 logits을 출력하고 최댓값을 선택해 예측 클래스를 얻거나 softmax 함수를 적용해 클래스별 예측 확률을 얻는다. 

그러나 대조적으로 모델의 확률의 출력을 텍스트로 변환하려면 디코딩 방법(Decoding Method)이 필요한데 여기에는 텍스트 생성에만 따르는 고유한 몇 가지 문제를 알아야될 필요가 있다. 
- 디코딩은 반복적으로 수행되므로 입력이 모델의 정방향 패스를 한번 통과할 때보다 많은 계산이 필요
- 생성된 텍스트의 품질과 다양성(diversity)은 디코딩 방법과 이에 관련된 하이퍼파라미터에 따라 달라짐

우선 디코딩이 어떻게 수행되는지 이해하기 위해 먼저 gpt-2의 사전 훈련 방법과 텍스트 생성에 적용하는 과정을 알아보자  

다른 __자기희귀 모델( )__ 또는 __코잘 언어모델__ 과 마찬가지로, GPT-2는 시작 프롬프트 또는 문맥 시퀀스 $\mathbf{x} = x_1, x_2, \ldots, x_k$가 주어질 때 텍스트에 등장하는 토큰 시퀀스 $\mathbf{y} = y_1, y_2, \ldots, y_t$의 확률 $\small\mathbf P(y|x)$를 추정하도록 사전 훈련된다. 직접 $\small \mathbf P(y|x)$를 추정하기 위해 충분한 훈련 데이터를 획득하기란 불가능하므로, 일반적으로 확률의 __연쇄 법칙(chain rule)__ 을 사용하여 __조건부 확률(conditional probabilities)__ 의 곱으로 나타낸다.


$$ \Large P(y_1, \ldots, y_t | \mathbf{x}) = \prod_{t=1}^{N} P(y_t | y_{< t}, \mathbf{x}) $$

[참고: Decoder Model 에서 출력 시퀀스를 생성하는 방식](https://jalammar.github.io/illustrated-transformer)  
[참고: Transformer의 Encoder는 입력 시퀀스를 시퀀스에 매팽하는 것](https://machinelearningmastery.com/the-transformer-model/)
